#!/usr/bin/env python3
"""
Automated Stream Verification Script

This script automatically manages service lifecycle and captures stream output for verification.
It handles process cleanup, service startup, stream capture, and result validation.

Usage:
    python scripts/verify_stream_output.py [--id REQUEST_ID]

Arguments:
    --id REQUEST_ID: Optional existing request ID to reuse for streaming
"""

import argparse
import json
import logging
import signal
import subprocess
import sys
import time
from pathlib import Path
from typing import Optional

import psutil
import requests

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
SERVICE_HOST = "http://localhost"
SERVICE_PORT = 8001  # Use 8001 as specified in requirements
HEALTH_ENDPOINT = "/api/health"
GENERATE_ENDPOINT = "/api/generate-code"
STREAM_ENDPOINT_TEMPLATE = "/api/generate-code/{}/stream"

# Timeouts and retries
HEALTH_CHECK_TIMEOUT = 10  # seconds
HEALTH_CHECK_RETRIES = 10
STREAM_TIMEOUT = 60  # seconds

# Output file
RESULT_FILE = "verify_stream_result.txt"


class StreamVerifier:
    """Automated stream verification handler."""

    def __init__(self, request_id: Optional[str] = None):
        self.request_id = request_id
        self.base_url = f"{SERVICE_HOST}:{SERVICE_PORT}"
        self.uvicorn_process: Optional[subprocess.Popen] = None

    def cleanup_port_processes(self, port: int) -> None:
        """Kill any processes using the specified port."""
        logger.info(f"æ¸…ç†ç«¯å£ {port} ä¸Šçš„è¿›ç¨‹...")

        try:
            # Find processes using the port
            for conn in psutil.net_connections():
                if conn.status == 'LISTEN' and conn.laddr.port == port:
                    try:
                        process = psutil.Process(conn.pid)
                        logger.info(f"ç»ˆæ­¢è¿›ç¨‹: {process.name()} (PID: {conn.pid})")
                        process.terminate()

                        # Wait for graceful termination
                        try:
                            process.wait(timeout=5)
                        except psutil.TimeoutExpired:
                            logger.warning(f"å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹: {conn.pid}")
                            process.kill()

                    except (psutil.NoSuchProcess, psutil.AccessDenied) as e:
                        logger.warning(f"æ— æ³•ç»ˆæ­¢è¿›ç¨‹ {conn.pid}: {e}")

        except Exception as e:
            logger.error(f"æ¸…ç†ç«¯å£è¿›ç¨‹å¤±è´¥: {e}")

    def start_service(self) -> bool:
        """Start the uvicorn service and wait for it to be ready."""
        logger.info("å¯åŠ¨ uvicorn æœåŠ¡...")

        # Clean up any existing processes on the ports
        self.cleanup_port_processes(8000)
        self.cleanup_port_processes(8001)

        try:
            # Start uvicorn process
            cmd = [
                sys.executable, "-m", "uvicorn",
                "src.main:app",
                "--host", "0.0.0.0",
                "--port", str(SERVICE_PORT),
                "--log-level", "info"
            ]

            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")

            self.uvicorn_process = subprocess.Popen(
                cmd,
                cwd=Path(__file__).parent.parent / "backend",  # Backend directory
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )

            # Wait for service to be ready
            logger.info("ç­‰å¾…æœåŠ¡å¯åŠ¨...")
            return self._wait_for_service_ready()

        except Exception as e:
            logger.error(f"å¯åŠ¨æœåŠ¡å¤±è´¥: {e}")
            self._cleanup_process()
            return False

    def _wait_for_service_ready(self) -> bool:
        """Poll the health endpoint until service is ready."""
        health_url = f"{self.base_url}{HEALTH_ENDPOINT}"

        for attempt in range(HEALTH_CHECK_RETRIES):
            try:
                logger.info(f"å¥åº·æ£€æŸ¥ (å°è¯• {attempt + 1}/{HEALTH_CHECK_RETRIES})...")

                response = requests.get(
                    health_url,
                    timeout=HEALTH_CHECK_TIMEOUT
                )

                if response.status_code == 200:
                    health_data = response.json()
                    if health_data.get("status") in ["healthy", "degraded"]:
                        logger.info("âœ… æœåŠ¡å·²å°±ç»ª")
                        return True

                logger.warning(f"æœåŠ¡æœªå°±ç»ªï¼ŒçŠ¶æ€ç : {response.status_code}")

            except requests.RequestException as e:
                logger.warning(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")

            time.sleep(2)

        logger.error("âŒ æœåŠ¡å¯åŠ¨è¶…æ—¶")
        return False

    def create_generation_request(self) -> Optional[str]:
        """Create a new code generation request."""
        generate_url = f"{self.base_url}{GENERATE_ENDPOINT}"

        try:
            logger.info("åˆ›å»ºæ–°çš„ä»£ç ç”Ÿæˆè¯·æ±‚...")

            payload = {
                "user_input": "Create a simple Python hello world program"
            }

            response = requests.post(
                generate_url,
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                data = response.json()
                request_id = data.get("request_id")
                logger.info(f"âœ… è¯·æ±‚åˆ›å»ºæˆåŠŸ: {request_id}")
                return request_id
            else:
                logger.error(f"âŒ åˆ›å»ºè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")

        except requests.RequestException as e:
            logger.error(f"åˆ›å»ºè¯·æ±‚å¤±è´¥: {e}")

        return None

    def capture_stream_output(self, request_id: str) -> bool:
        """Capture stream output to file."""
        stream_url = f"{self.base_url}{STREAM_ENDPOINT_TEMPLATE.format(request_id)}"

        logger.info(f"è¿æ¥æµå¼æ¥å£: {stream_url}")
        logger.info(f"è¾“å‡ºå°†ä¿å­˜åˆ°: {RESULT_FILE}")

        try:
            # Clear result file
            with open(RESULT_FILE, 'w', encoding='utf-8') as f:
                f.write("")

            # Connect to stream with timeout
            response = requests.get(
                stream_url,
                stream=True,
                timeout=STREAM_TIMEOUT
            )

            if response.status_code != 200:
                logger.error(f"âŒ æµå¼è¿æ¥å¤±è´¥: {response.status_code}")
                logger.error(f"å“åº”å†…å®¹: {response.text[:500]}...")
                return False

            logger.info("âœ… æµå¼è¿æ¥æˆåŠŸï¼Œå¼€å§‹æ•è·æ•°æ®...")

            # Capture SSE events
            captured_data = []
            start_time = time.time()
            current_event = {}
            content_buffer = ""

            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8').strip()

                    # Parse SSE format
                    if line_str.startswith('event: '):
                        # Save previous event if exists
                        if current_event and 'data' in current_event:
                            try:
                                json_data = json.loads(current_event['data'])
                                captured_data.append(json_data)

                                # Write to file immediately
                                with open(RESULT_FILE, 'a', encoding='utf-8') as f:
                                    f.write(f"{json.dumps(json_data, ensure_ascii=False)}\n")

                                logger.info(f"æ”¶åˆ°äº‹ä»¶ {current_event.get('event', 'unknown')}: {current_event['data'][:100]}...")

                            except json.JSONDecodeError as e:
                                logger.warning(f"JSONè§£æå¤±è´¥: {e}, æ•°æ®: {current_event['data'][:200]}...")

                        # Start new event
                        current_event = {'event': line_str[7:]}  # Remove 'event: '

                    elif line_str.startswith('data: '):
                        # Accumulate data lines
                        data_content = line_str[6:]  # Remove 'data: '
                        if 'data' not in current_event:
                            current_event['data'] = data_content
                        else:
                            current_event['data'] += data_content

                    elif line_str == '':
                        # Empty line indicates end of event
                        if current_event and 'data' in current_event:
                            try:
                                json_data = json.loads(current_event['data'])
                                captured_data.append(json_data)

                                # Write to file immediately
                                with open(RESULT_FILE, 'a', encoding='utf-8') as f:
                                    f.write(f"{json.dumps(json_data, ensure_ascii=False)}\n")

                                logger.info(f"æ”¶åˆ°äº‹ä»¶ {current_event.get('event', 'unknown')}: {current_event['data'][:100]}...")

                            except json.JSONDecodeError as e:
                                logger.warning(f"JSONè§£æå¤±è´¥: {e}, æ•°æ®: {current_event['data'][:200]}...")

                        current_event = {}

                # Check for timeout
                if time.time() - start_time > STREAM_TIMEOUT:
                    logger.warning(f"æµå¼è¾“å‡ºè¶…æ—¶ ({STREAM_TIMEOUT}s)")
                    break

            # Handle any remaining event
            if current_event and 'data' in current_event:
                try:
                    json_data = json.loads(current_event['data'])
                    captured_data.append(json_data)

                    with open(RESULT_FILE, 'a', encoding='utf-8') as f:
                        f.write(f"{json.dumps(json_data, ensure_ascii=False)}\n")

                    logger.info(f"æ”¶åˆ°æœ€ç»ˆäº‹ä»¶ {current_event.get('event', 'unknown')}: {current_event['data'][:100]}...")

                except json.JSONDecodeError as e:
                    logger.warning(f"æœ€ç»ˆäº‹ä»¶JSONè§£æå¤±è´¥: {e}")

            logger.info(f"æµå¼æ•è·å®Œæˆï¼Œå…±æ”¶åˆ° {len(captured_data)} ä¸ªæ•°æ®å—")

            # Check if we got any meaningful data
            return len(captured_data) > 0

        except requests.RequestException as e:
            logger.error(f"æµå¼è¿æ¥é”™è¯¯: {e}")
            return False

    def verify_results(self) -> bool:
        """Verify that the captured results contain expected content."""
        logger.info("éªŒè¯ç»“æœæ–‡ä»¶...")

        try:
            with open(RESULT_FILE, 'r', encoding='utf-8') as f:
                content = f.read()

            if not content.strip():
                logger.error("âŒ ç»“æœæ–‡ä»¶ä¸ºç©º")
                return False

            # Check for expected JSON fields
            checks = [
                '"phase":' in content,
                '"content":' in content,
                '"implement"' in content or '"phase"' in content
            ]

            passed_checks = sum(checks)
            total_checks = len(checks)

            logger.info(f"éªŒè¯ç»“æœ: {passed_checks}/{total_checks} é€šè¿‡")

            if all(checks):
                logger.info("âœ… éªŒè¯é€šè¿‡ - æ‰¾åˆ°å…³é”®å­—æ®µ")
                return True
            else:
                logger.warning("âŒ éªŒè¯å¤±è´¥ - ç¼ºå°‘å…³é”®å­—æ®µ")
                logger.info(f"æ–‡ä»¶å†…å®¹é¢„è§ˆ: {content[:500]}...")
                return False

        except FileNotFoundError:
            logger.error("âŒ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨")
            return False
        except Exception as e:
            logger.error(f"éªŒè¯å¤±è´¥: {e}")
            return False

    def _cleanup_process(self) -> None:
        """Clean up the uvicorn process."""
        if self.uvicorn_process and self.uvicorn_process.poll() is None:
            logger.info("æ¸…ç† uvicorn è¿›ç¨‹...")
            try:
                self.uvicorn_process.terminate()
                try:
                    self.uvicorn_process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    logger.warning("å¼ºåˆ¶ç»ˆæ­¢ uvicorn è¿›ç¨‹")
                    self.uvicorn_process.kill()
            except Exception as e:
                logger.error(f"æ¸…ç†è¿›ç¨‹å¤±è´¥: {e}")

    def run_verification(self) -> bool:
        """Run the complete verification process."""
        try:
            logger.info("ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–æµå¼éªŒè¯...")

            # 1. Start service
            if not self.start_service():
                logger.error("âŒ æœåŠ¡å¯åŠ¨å¤±è´¥")
                return False

            # 2. Get or create request ID
            if self.request_id:
                logger.info(f"ä½¿ç”¨æä¾›çš„è¯·æ±‚ID: {self.request_id}")
                request_id = self.request_id
            else:
                logger.info("åˆ›å»ºæ–°çš„ä»£ç ç”Ÿæˆè¯·æ±‚...")
                request_id = self.create_generation_request()
                if not request_id:
                    logger.error("âŒ åˆ›å»ºè¯·æ±‚å¤±è´¥")
                    return False

            # 3. Capture stream output
            if not self.capture_stream_output(request_id):
                logger.error("âŒ æµå¼è¾“å‡ºæ•è·å¤±è´¥")
                return False

            # 4. Verify results
            if not self.verify_results():
                logger.error("âŒ ç»“æœéªŒè¯å¤±è´¥")
                return False

            logger.info("ğŸ‰ éªŒè¯æˆåŠŸå®Œæˆï¼")
            return True

        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...")
            return False
        except Exception as e:
            logger.error(f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
            return False
        finally:
            self._cleanup_process()


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Automated Stream Verification Script")
    parser.add_argument(
        "--id",
        type=str,
        help="Optional existing request ID to reuse for streaming"
    )

    args = parser.parse_args()

    # Check if psutil is available
    try:
        import psutil
    except ImportError:
        logger.error("âŒ éœ€è¦å®‰è£… psutil: pip install psutil")
        sys.exit(1)

    # Check if requests is available
    try:
        import requests
    except ImportError:
        logger.error("âŒ éœ€è¦å®‰è£… requests: pip install requests")
        sys.exit(1)

    # Run verification
    verifier = StreamVerifier(request_id=args.id)
    success = verifier.run_verification()

    # Exit with appropriate code
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
